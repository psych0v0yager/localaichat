{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jfTDBnMbGO3"
      },
      "source": [
        "# Efficient Coding Assistant with simpleaichat\n",
        "\n",
        "_Updated using `gpt-3.5-turbo-0613`_\n",
        "\n",
        "Many coders use ChatGPT for coding help, however the web interface can be slow and contain unnecessary discussion when you want code. With some system prompt engineering and simplechatapi streaming, you can cut down code time generation and costs significantly.\n",
        "\n",
        "**DISCLAIMER: Your mileage may vary in terms of code quality and accuracy in practice, but this is a good, hackable starting point.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5ZeSKyedacCE"
      },
      "outputs": [],
      "source": [
        "from localaichat.localaichat import AIChat, AsyncAIChat\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kXTv7zXapit"
      },
      "source": [
        "For the following cell, input your OpenAI API key when prompted. **It will not be saved to the notebook**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_3QIHtnaqdw",
        "outputId": "334508c7-d1da-4270-b98d-359da120ceff"
      },
      "outputs": [],
      "source": [
        "api_key = \"token-abc123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mxBkiR9FacCF"
      },
      "outputs": [],
      "source": [
        "# params = {\"temperature\": 0.0}  # for reproducibility\n",
        "# model = \"gpt-3.5-turbo\"  # in production, may want to use model=\"gpt-4\" if have access\n",
        "\n",
        "ai = AIChat(client_type=\"vLLM\", api_key=\"token-abc123\", model=\"/home/server_runner/Desktop/CloserModels/llama3_70B_awq\", system=\"You are a helpful assistant.\", console=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4eVxf3Jawo_"
      },
      "source": [
        "Let's start with a simple `is_palindrome()` function in Python, and track how long it takes to run. The output of this should be similar to what is shown in the ChatGPT webapp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCkboiHsacCG",
        "outputId": "1608a19a-23e2-4555-c096-98d5bd08f51f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a simple implementation of an `is_palindrome()` function in Python:\n",
            "```\n",
            "def is_palindrome(s):\n",
            "    \"\"\"\n",
            "    Returns True if the input string is a palindrome, False otherwise.\n",
            "    \"\"\"\n",
            "    return s == s[::-1]\n",
            "```\n",
            "Here's an explanation of how it works:\n",
            "\n",
            "* `s[::-1]` is a slice that reverses the input string `s`. For example, if `s` is `\"hello\"`, then `s[::-1]` would be `\"olleh\"`.\n",
            "* We then compare the original string `s` with its reversed version using the `==` operator. If they are the same, then the string is a palindrome, and we return `True`. Otherwise, we return `False`.\n",
            "\n",
            "Here's an example usage:\n",
            "```\n",
            ">>> is_palindrome(\"madam\")\n",
            "True\n",
            ">>> is_palindrome(\"hello\")\n",
            "False\n",
            "```\n",
            "Note that this implementation is case-sensitive and considers spaces and punctuation as part of the string. If you want to ignore case, spaces, and punctuation, you can modify the function accordingly. For example:\n",
            "```\n",
            "def is_palindrome(s):\n",
            "    s = s.lower().replace(\" \", \"\").replace(\",\", \"\").replace(\".\", \"\")\n",
            "    return s == s[::-1]\n",
            "```\n",
            "This version of the function converts the input string to lowercase, removes spaces, commas, and periods, and then checks if the resulting string is a palindrome.\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 0 ns, sys: 2.36 ms, total: 2.36 ms\n",
            "Wall time: 13.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = ai(\"Write an is_palindrome() function in Python.\")\n",
        "print(response)\n",
        "print(\"\\n\\n\")  # separate time from generated text for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBOCM4LMbz3C"
      },
      "source": [
        "That's the typical implementation. However, there's a trick to cut the processing time in half, well known by technical hiring managers who want to trip up prospective candidates.\n",
        "\n",
        "ChatGPT outputs the statistically most common implementation, but it's not necessairily the best. A second pass allows ChatGPT to refine its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwsfrcPHacCH",
        "outputId": "dcbfdce5-8f72-4d00-cfe9-6eef41018639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a more efficient implementation of the `is_palindrome()` function:\n",
            "```\n",
            "def is_palindrome(s):\n",
            "    \"\"\"\n",
            "    Returns True if the input string is a palindrome, False otherwise.\n",
            "    \"\"\"\n",
            "    left, right = 0, len(s) - 1\n",
            "    while left < right:\n",
            "        if s[left] != s[right]:\n",
            "            return False\n",
            "        left += 1\n",
            "        right -= 1\n",
            "    return True\n",
            "```\n",
            "Here's what's changed:\n",
            "\n",
            "* Instead of creating a reversed copy of the entire string (`s[::-1]`), we use two pointers, `left` and `right`, to iterate through the string from both ends.\n",
            "* We compare the characters at the `left` and `right` indices, and if they don't match, we immediately return `False`.\n",
            "* If the characters match, we increment `left` and decrement `right` to move towards the center of the string.\n",
            "* If the loop completes without finding a mismatch, we return `True`, indicating that the string is a palindrome.\n",
            "\n",
            "This implementation has a time complexity of O(n/2), where n is the length of the input string, because we only need to iterate through half of the string to determine if it's a palindrome. This is more efficient than the previous implementation, which had a time complexity of O(n) due to the creation of a reversed copy of the entire string.\n",
            "\n",
            "Note that this implementation still has a space complexity of O(1) because we only use a few extra variables to store the indices and don't allocate any additional memory that scales with the input size.\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 2.34 ms, sys: 0 ns, total: 2.34 ms\n",
            "Wall time: 16.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = ai(\"Make it more efficient.\")\n",
        "print(response)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWRKL30uacCH",
        "outputId": "ac47340b-9c7a-4517-a36a-d4fb92765311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "980"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai.total_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okqqoe7lcqUH"
      },
      "source": [
        "In all, it took ~6 seconds and utilized 511 tokens. But there's a lot of unnecessary natter in the output:\n",
        "\n",
        "- The conversational preamble before the code\n",
        "- Docstrings and code comments\n",
        "- A long explanation of the code which may be redundant to the above\n",
        "\n",
        "All this natter adds latency and cost.\n",
        "\n",
        "The easiest technique to guide AI text generation is to use **prompt engineering**, specifically to give it a new system prompt to say precisely what you want. As of June 27th 2023, the default ChatGPT API responds very well to commands.\n",
        "\n",
        "Now, for the new `system` prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iA9AASKfacCH"
      },
      "outputs": [],
      "source": [
        "system_optimized = \"\"\"Write a Python function based on the user input.\n",
        "\n",
        "You must obey ALL the following rules:\n",
        "- Only respond with the Python function.\n",
        "- Never put in-line comments or docstrings in your code.\"\"\"\n",
        "\n",
        "# ai_2 = AIChat(api_key=api_key, system=system_optimized, model=model, params=params)\n",
        "ai_2 = AIChat(client_type=\"vLLM\", api_key=\"token-abc123\", model=\"/home/server_runner/Desktop/CloserModels/llama3_70B_awq\", system=system_optimized, console=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRYfniAXacCI",
        "outputId": "fecb6928-a1e8-48f6-c336-5055cbca7a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def is_palindrome(s):\n",
            "    return s == s[::-1]\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 2.11 ms, sys: 89 μs, total: 2.2 ms\n",
            "Wall time: 836 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = ai_2(\"is_palindrome\")\n",
        "print(response)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGJr-GfIacCI",
        "outputId": "68a555bd-d3e0-4b39-cca6-a11ce6ad1455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def is_palindrome(s):\n",
            "    i, j = 0, len(s) - 1\n",
            "    while i < j:\n",
            "        if s[i] != s[j]:\n",
            "            return False\n",
            "        i += 1\n",
            "        j -= 1\n",
            "    return True\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 2.2 ms, sys: 0 ns, total: 2.2 ms\n",
            "Wall time: 2.81 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = ai_2(\"Make it more efficient.\")\n",
        "print(response)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eMDXGysacCJ",
        "outputId": "0d6272c1-cb2a-460b-fe37-09c896b119e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "214"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_2.total_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua28VqKteKkr"
      },
      "source": [
        "~3 seconds total with 190 tokens used: that's 2x faster at 1/3 the cost!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaE3ftkMacCJ"
      },
      "source": [
        "## Create a Function\n",
        "\n",
        "Now we can create a function to automate the two calls we did above for any arbitrary input.\n",
        "\n",
        "For each call, we'll create an independent temporary session within simpleaichat and then clean it up. We'll also use a regex to strip unneded backticks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mN_-3fwuacCL"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "import re\n",
        "\n",
        "ai_func = AIChat(client_type=\"vLLM\", api_key=\"token-abc123\", model=\"/home/server_runner/Desktop/CloserModels/llama3_70B_awq\", console=False)\n",
        "def gen_code(query):\n",
        "    id = uuid4()\n",
        "    # ai_func.new_session(api_key=api_key, id=id, system=system_optimized, params=params, model=model)\n",
        "    ai_func.new_session(client_type=\"vLLM\", api_key=\"token-abc123\", model=\"/home/server_runner/Desktop/CloserModels/llama3_70B_awq\", system=system_optimized, console=False, id=id)\n",
        "    _ = ai_func(query, id=id)\n",
        "    response_optimized = ai_func(\"Make it more efficient.\", id=id)\n",
        "\n",
        "    ai_func.delete_session(id=id)\n",
        "    return response_optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIkdpBwOacCL",
        "outputId": "f00c393b-87c0-4bc6-9c35-d323c6de6812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def is_palindrome(s):\n",
            "    i, j = 0, len(s) - 1\n",
            "    while i < j:\n",
            "        if s[i] != s[j]:\n",
            "            return False\n",
            "        i += 1\n",
            "        j -= 1\n",
            "    return True\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 0 ns, sys: 4.76 ms, total: 4.76 ms\n",
            "Wall time: 3.62 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "code = gen_code(\"is_palindrome\")\n",
        "print(code)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1LjZjFBacCM",
        "outputId": "53d96baf-1f6b-46fb-bba8-5eda772d8c87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def reverse_string(s):\n",
            "    return ''.join(reversed(s))\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 0 ns, sys: 2.66 ms, total: 2.66 ms\n",
            "Wall time: 1.57 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "code = gen_code(\"reverse string\")\n",
        "print(code)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdSNUiapacCM",
        "outputId": "207634e0-ef48-4428-a3c0-15a80ad845f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def pretty_print_dict(d, indent=0):\n",
            "    for key, value in d.items():\n",
            "        print('  ' * indent + str(key))\n",
            "        if isinstance(value, dict):\n",
            "            pretty_print_dict(value, indent + 1)\n",
            "        else:\n",
            "            print('  ' * (indent + 1) + str(value), end='\\n\\n')\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 567 μs, sys: 2.11 ms, total: 2.68 ms\n",
            "Wall time: 7.02 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "code = gen_code(\"pretty print dict\")\n",
        "print(code)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ppyZrFTacCM",
        "outputId": "a4e04c60-4681-4d7f-b846-028a9b1271f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "import cv2\n",
            "\n",
            "def load_and_flip_image_horizontally(image_path):\n",
            "    return cv2.flip(cv2.imread(image_path), 1)\n",
            "```\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 3.01 ms, sys: 0 ns, total: 3.01 ms\n",
            "Wall time: 3.96 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "code = gen_code(\"load and flip image horizontally\")\n",
        "print(code)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNQhAKFqacCM",
        "outputId": "fcfc568f-0777-4df4-a8f7-20fa0e4e34d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "import hashlib\n",
            "import multiprocessing\n",
            "\n",
            "def hash_file(file_path):\n",
            "    hash_object = hashlib.sha256()\n",
            "    with open(file_path, 'rb') as f:\n",
            "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
            "            hash_object.update(chunk)\n",
            "    return file_path, hash_object.hexdigest()\n",
            "\n",
            "def multiprocess_hash(file_paths):\n",
            "    with multiprocessing.Pool() as pool:\n",
            "        return pool.map(hash_file, file_paths)\n",
            "```\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 2.58 ms, sys: 0 ns, total: 2.58 ms\n",
            "Wall time: 8.12 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "code = gen_code(\"multiprocess hash\")\n",
        "print(code)\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbQspzba2_GO"
      },
      "source": [
        "## Generating Optimized Code in a Single API Call w/ Structured Output Data\n",
        "\n",
        "Sometimes you may not want to make two calls to OpenAI. One hack you can do is to define an expected structured output to tell it to sequentially generate the normal code output, then the optimized output.\n",
        "\n",
        "This structure is essentially a different form of prompt engineering, but you can combine it with a system prompt if needed.\n",
        "\n",
        "This will also further increase response speed, but may not necessairly result in fewer tokens used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lFCLUODV28FZ"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "import orjson\n",
        "\n",
        "class write_python_function(BaseModel):\n",
        "    \"\"\"Writes a Python function based on the user input.\"\"\"\n",
        "    code: str = Field(description=\"Python code\")\n",
        "    efficient_code: str = Field(description=\"More efficient Python code than previously written\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6rTqftko4N2L"
      },
      "outputs": [],
      "source": [
        "# ai_struct = AIChat(api_key=api_key, console=False, model=model, params=params, save_messages=False)\n",
        "ai_struct = AIChat(client_type=\"vLLM\", api_key=\"token-abc123\", model=\"/home/server_runner/Desktop/CloserModels/llama3_70B_awq\", system=\"You are a helpful assistant.\", console=False, save_messages=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ielJLnz4e5U",
        "outputId": "d9b3f315-1840-4781-b922-f0cd889a4086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"code\": \"def is_palindrome(s):\\n    return s == s[::-1]\",\n",
            "  \"efficient_code\": \"def is_palindrome(s):\\n    n = len(s)\\n    for i in range(n // 2):\\n        if s[i] != s[n - i - 1]:\\n            return False\\n    return True\"\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 131 ms, sys: 1.39 ms, total: 133 ms\n",
            "Wall time: 1.67 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response_structured = ai_struct(\"is_palindrome\", output_schema=write_python_function)\n",
        "\n",
        "# orjson.dumps preserves field order from the ChatGPT API\n",
        "print(orjson.dumps(response_structured, option=orjson.OPT_INDENT_2).decode())\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8sshigf53Nq"
      },
      "source": [
        "As evident, the output is a `dict` so you'd just return the `efficient_code` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIKI_8aq51RH",
        "outputId": "ab239077-3114-4b52-a127-0c5a93308627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def is_palindrome(s):\n",
            "    n = len(s)\n",
            "    for i in range(n // 2):\n",
            "        if s[i] != s[n - i - 1]:\n",
            "            return False\n",
            "    return True\n"
          ]
        }
      ],
      "source": [
        "print(response_structured[\"efficient_code\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KThchw1S7MwS",
        "outputId": "059aedeb-13c9-4802-a0cf-239ff7384722"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_struct.total_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQcNrZcK7bn1"
      },
      "source": [
        "Token-wise it's about the same, but there's a significant speedup in generation for short queries such as these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2tfYs3a6oBy"
      },
      "source": [
        "Trying the other examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrJtaW5m7Cdy",
        "outputId": "a74621e8-cebc-495b-adaa-bec926de36dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def reverse_string(s):\n",
            "    return ''.join(reversed(s))\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 24.6 ms, sys: 250 µs, total: 24.9 ms\n",
            "Wall time: 1.37 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response_structured = ai_struct(\"reverse string\", output_schema=write_python_function)\n",
        "print(response_structured[\"efficient_code\"])\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJxRji6h8Gww",
        "outputId": "45dbf068-b9b2-4d04-fccb-f988eda5538f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from PIL import Image\n",
            "\n",
            "def load_and_flip_image_horizontally(image_path):\n",
            "    return Image.open(image_path).transpose(Image.FLIP_LEFT_RIGHT)\n",
            "\n",
            "\n",
            "\n",
            "CPU times: user 15.4 ms, sys: 2.15 ms, total: 17.6 ms\n",
            "Wall time: 1.79 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response_structured = ai_struct(\"load and flip image horizontally\", output_schema=write_python_function)\n",
        "print(response_structured[\"efficient_code\"])\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-QaAsRXbhAj"
      },
      "source": [
        "## MIT License\n",
        "\n",
        "Copyright (c) 2023 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
